import torch
import torch.nn.functional as F
from tqdm import tqdm
from monai.transforms import Resize
from eval_v2 import dice_score_v2

def train(model, dataloader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    total_dice = 0
    total_dice_v2 = 0 #ì›ë˜ ë‚´ dice score ê³„ì‚° ê°’
    num_batches = len(dataloader)

    loop = tqdm(dataloader, desc="ğŸš‚ Train", leave=True)

    for images, masks in loop:
        images, masks = images.to(device), masks.to(device)

        optimizer.zero_grad()
        preds, _ = model(images)  # preds: [B, C, 128, 128, 32]

        # âœ… Resize (1íšŒë§Œ ì •ì˜í•˜ì—¬ ë°˜ë³µë¬¸ì—ì„œ ì¬ì‚¬ìš©) â†’ ì†ë„ ê°œì„ 
        target_spatial = list(masks.shape[-3:])  # [D, H, W]
        resize_fn = Resize(spatial_size=target_spatial, mode="trilinear")

        # âš ï¸ ê° ë°°ì¹˜ sampleë³„ Resize ì ìš© (MONAIëŠ” [C, D, H, W]ë§Œ ì§€ì›)
        preds_upsampled = torch.stack([resize_fn(p) for p in preds])  # [B, C, 240, 240, 155]

        # ğŸ¯ Loss ê³„ì‚° (CrossEntropy ê¸°ì¤€: logits ì‚¬ìš© â†’ softmax X)
        loss = criterion(preds_upsampled, masks)
        loss.backward()
        optimizer.step()

        # ğŸ¯ Dice ê³„ì‚° (softmax í¬í•¨ëœ í•¨ìˆ˜ ì‚¬ìš©)
        preds_soft = torch.softmax(preds_upsampled, dim=1)
        dice_v2 = dice_score_v2(preds_upsampled, masks)
        # dice_v2 = multiclass_dice(preds_soft, masks)

        total_loss += loss.item()
        total_dice_v2 += dice_v2.item()
        loop.set_postfix(loss=loss.item(), dice_v2=dice_v2.item())

    avg_loss = total_loss / num_batches
    # avg_dice = total_dice / num_batches
    avg_dice_v2 = total_dice_v2 / num_batches
    return avg_loss, avg_dice_v2
